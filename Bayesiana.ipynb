{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2eca296-4692-4cc4-9238-d81ba5cd1d78",
   "metadata": {},
   "source": [
    "# Regresión lineal Bayesiana "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b35c03-5bf3-4b0b-aaed-dfa6df257da3",
   "metadata": {},
   "source": [
    "<img src=\"https://th.bing.com/th/id/R.3bff0ba279f0ed7d508c5e7da683af52?rik=Z2ty6vcIEsLQLw&riu=http%3a%2f%2fblogs.upn.edu.pe%2fcomunicaciones%2fwp-content%2fuploads%2fsites%2f3%2f2016%2f07%2fupn_blog_com_era-digital-sociedad-conocimiento_21jul.jpg&ehk=Fwupl%2beIQQwF9K58bfG4lVRr9EbtqhK0373Mr%2bC%2bkdU%3d&risl=&pid=ImgRaw&r=0=5x5\" width=\"500\" height=\"500\">\n",
    "\n",
    "[Fuente](https://blogs.upn.edu.pe/comunicaciones/2016/07/21/digital-sociedad-del-conocimiento/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef086a00-006e-4680-878c-4ea7059a82fa",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3963e9-0a29-4f0e-a7f8-cdffc8fc0b6c",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "La regresión lineal Bayesiana se  refiere a un tipo de regresión lineal  que se basa en la estadística bayesiana, es decir, en el teorema de Bayes.\n",
    "La regresión lineal bayesiana es un tipo de modelado condicional en el que la media de una variable se describe mediante una combinación lineal de otras variables, con el objetivo de obtener la probabilidad posterior de los coeficientes de regresión (así como otros parámetros que describen la distribución de la regresora).) y, en última instancia, permitir la predicción fuera de la muestra del regresor (a menudo etiquetado como y) condicional a los valores observados de los regresores (generalmente X). La versión más simple y más utilizada de este modelo es el modelo lineal normal, en el que ydadoXse distribuye Gaussiana. En este modelo, y bajo una elección particular de probabilidades previas para los parámetros, los llamados anteriores conjugados, el posterior se puede encontrar analíticamente. Con priores elegidos más arbitrariamente, los posteriores generalmente tienen que ser aproximados.\n",
    "En este modelo, y bajo una elección particular de probabilidades previas para los parámetros, los llamados priores conjugados, la posterior se puede encontrar analíticamente. Con priores elegidos más arbitrariamente, los posteriores generalmente tienen que ser aproximados.[Wikipedia](https://en.wikipedia.org/wiki/Bayesian_linear_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd546d8-e3ab-4666-af21-5817269aea76",
   "metadata": {},
   "source": [
    "## Nota Histórica\n",
    "Las aportaciones de Turing a la estadística Bayesiana se desarrollaron en relación con el algoritmo “bamburismus” que sirvió para desencriptar los mensajes enviados por la armada naval germana, durante la Segunda Guerra Mundial, usando la máquina Enigma. Dichos mensajes eran de capital importancia para la población británica cuyo abastecimiento dependía de manera crítica de la supervivencia de los convoyes marítimos aliados.\n",
    "\n",
    "El famoso estadístico Bayesiano, Irwing J. Good, quien a principios de los años 40 del siglo pasado trabajaba como ayudante de Turing en Bletchley Park, explica en un artículo publicado en Biometrika (I. J. Good, 1979) las aportaciones metodológicas de Turing a la teoría Bayesiana, tanto al denominado peso de la evidencia como a la introducción de un test de hipótesis basado en la razón de verosimilitudes con el que confrontar hipótesis nulas y alternativas.  Gracias a recientes desclasificaciones de documentos relacionados con el “bamburismus” por parte del gobierno americano, conocemos que dicho algoritmo se fundamenta en el test de hipótesis diseñado por Turing.[Fuente](https://blogs.elpais.com/turing/2012/12/alan-turing-y-la-estadistica-bayesiana.html)\n",
    "\n",
    "<img src=\"https://th.bing.com/th/id/R.5f9ace9f628d3586696b562562f891f5?rik=SK6fuTfqjUmUlw&pid=ImgRaw&r=0\" width=\"200\" height=\"200\">\n",
    "\n",
    "[Fuente](http://laverdadteharalibre6.blogspot.com/2014/03/alan-turing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1873f49-8936-43c0-adb1-5d8fbab0a1bc",
   "metadata": {},
   "source": [
    "## Fundamento Matemático"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f31c1c-9f12-4a65-9ae2-e2b32e76a340",
   "metadata": {},
   "source": [
    "En general conocemos el modelo de regresión lineal como una expresión dada de la siguiente manera:$$Y=X\\beta + \\epsilon$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699a3bb-2495-4e69-adab-f31d5c61ef12",
   "metadata": {},
   "source": [
    "Donde $Y$ es el vector de variables dependientes o de respuesta, $X$  es la matriz de variables independientes, $\\beta$ corresponde al vector de coefientes y $\\epsilon$ corresponde al vector de errores no observables el cual se supone con distribucion normal con media $\\mu=0$ y matriz de varianza y covarianza  $\\sigma^2 I_n$, es decir $\\epsilon \\~ N(0,\\sigma^2 I_n)$.Por lo que $Y~N_n (X\\beta,\\sigma^2 I_n)$.\n",
    "\n",
    "Ahora bien, teniendon cuenta que de acuerdo a la  estadística Bayesiana, los parámetros también son variables aleatorias es decir los parámetros del modelo no corresponden a estimaciones puntuales sino a distribuciones de probabilidad. Y que además la regresión Bayesiana al estar basada en el fundamento teórico de la estadística Bayesiana está entonces basada en el Teorema de Bayes, el cual se resume de la siguiente manera:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951af137-ace1-427d-bd66-32922c6a415e",
   "metadata": {},
   "source": [
    "### Teorema de Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9ad85-ea59-40c7-8be3-841fd841e52a",
   "metadata": {},
   "source": [
    "El teorema de Bayes, en la teoría de la probabilidad, es una proposición planteada por el matemático inglés Thomas Bayes (1702-1761) y publicada póstumamente en 1763, que expresa la probabilidad condicional de un evento aleatorio A dado B en términos de la distribución de probabilidad condicional del evento B dado A y la distribución de probabilidad marginal de solo A.\n",
    "\n",
    "Sea $\\{A_1,A_2,...,A_n\\}$ donde $P(A_i)\\neq 0$. Si se conoce $P(B|A_i)$ entonces:\n",
    "\n",
    "$$P(A_i|B)=\\frac{P(B|A_i)P(A_i)}{P(B)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e54d7-94e3-4c9e-9826-6ac2e7f1d778",
   "metadata": {},
   "source": [
    "#### Ejemplo de aplicación del teorema de Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd029954-707f-4f4e-8737-9328d9632d74",
   "metadata": {},
   "source": [
    " Suponga que el $5\\%$ de  la población de Bogotá tiene una enfermedad. La probabilidad de que el individuo elegido de entre la gente padezca la enfermedad es de $0.05$. Suponga que la evidencia histórica muestra que si una persona padece realmente la enfermedad, la probabilidad de que la prueba indique la presencia de la enfermedad en una persona que en realidad no la padece es de $0.15$. Elija al azar a una persona de Bogotá y aplique la prueba. Los resultados de la prueba indican que la enfermedad está presente. ¿Cuál es la probabilidad de que la persona en realidad padezca la enfermedad?\n",
    "\n",
    "**Solución**\n",
    "\n",
    "Sean: $A_1$ el evento \"padece la enfermedad\", $A_2$ el evento \"no padece la enfermedad\".\n",
    "\n",
    "$$P(A_1)=0.05---> \\textbf{Probabilidad a priori}$$\n",
    "$$P(A_2)=1-0.05=0.95$$\n",
    "$$P(B|A_1)=0.9$$\n",
    "$$P(B|A_2)=0.15$$\n",
    "\n",
    "*Probabilidad a posteriori*\n",
    "\n",
    "$$P(A_1|B)=\\frac{P(A_1)P(B|A_1)}{P(A_1)P(B|A_1)+P(A_2)P(B|A_2)}=\\frac{(0.5)(0.9)}{(0.05)(0.9)+(0.95)(0.15)}=\\frac{0.045}{0.1875}=0.24$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bde87b-d9cb-4352-8bb4-19a7892dea04",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384941f0-530f-4746-b526-f688e85b71fa",
   "metadata": {},
   "source": [
    "En la estadística Bayesiana se supone que tanto parámetros ($\\mathbf{\\theta}$) como observaciones ($\\mathbf{x}$) son variables aleatorias. Cuando se realiza un experimento de tipo estadístico, las observaciones se convierten en datos que podemos ver. Por otro lado, los parámetros se convierten en cantidades (parámetros numéricos) que no podemos observar directamente. Solamente podemos **inferir** el valor del parámetro a partir de los datos. Nótese que datos y los *parámetros numéricos* en el experimento son realizaciones de variables aleatorias. Los parámetros puede ser llamados variables latentes, en el sentido que están presentes y determinan las observaciones, pero no pueden ser observadas directamente.\n",
    "\n",
    "\n",
    "[Fuente:AP](https://github.com/AprendizajeProfundo/Estadistica-Bayesiana/blob/master/cuadernos/Introducci%C3%B3n_Estadistica_Bayesiana.ipynb)\n",
    "\n",
    "\n",
    "En el enfoque bayesiano, los datos se complementan con información adicional en forma de una distribución de probabilidad previa. La creencia previa sobre los parámetros se combina con la función de probabilidad de los datos de acuerdo con el teorema de Bayes para producir la creencia posterior sobre los parámetros $\\beta$, $\\sigma$ . El anterior puede tomar diferentes formas funcionales dependiendo del dominio y de la información de la que se disponga a priori."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31cff8-a44a-4438-aa19-601b7ce0b210",
   "metadata": {},
   "source": [
    "#### Distribuciones a priori no informativas\n",
    "**¿Cómo se puede justificar la elección de una distribución a priori uniforme para θ?**\n",
    "\n",
    "\n",
    "La distribución a priori debe incluir todos los valores posibles de θ, pero no tiene que estar necesariamente concentrada en torno al verdadero valor ya que la información en los datos modificará y dominará cualquier especificación probabilística inicial.\n",
    "El razonamiento de Laplace para la densidad a priori uniforme es llamado principio de la razón insuficiente, el cual indica que si nada es conocido acerca de θ, entonces no hay ninguna razón para asignar probabilidades diferentes a algunos de sus valores.\n",
    "[Fuente: Ms Carlos López](http://tarwi.lamolina.edu.pe/~clopez/Estadistica_Bayesiana/Modelos_uniparametricos.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff01b2e-00e0-4405-9626-a9996f0cd6bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Distribución predictiva posterior\n",
    "\n",
    "La distribución predictiva es la distribución de observaciones futuras dada la muestra actual.\n",
    "\n",
    "Supongamos que $ y_ {n + 1} $ es una observación futura que es independiente de los datos observados $ \\mathbf{y}$, dado el correspondiente parámetro $\\boldsymbol{\\theta}$. \n",
    "\n",
    "\n",
    "Entonces, la distribución predictiva para $ y_{n + 1} $ viene dada por\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p(y_{n+1}|\\mathbf{y}).\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Observe que\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y_{n+1}|\\mathbf{y})&=\\int\n",
    "p(y_{n+1},\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta}\\\\\n",
    "&= \\int\n",
    "f(y_{n+1}|\\mathbf{\\theta},\\mathbf{y})p(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta}\\\\\n",
    "&=\\int\n",
    "f(y_{n+1}|\\mathbf{\\theta})p(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Es decir que\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p(y_{n+1}|\\mathbf{y}) = \\int\n",
    "f(y_{n+1}|\\mathbf{\\theta})p(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta},\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "y en consecuencia los valores predichos pueden ser calculados dentro del algorimo de muestreo.  Supongamos que en el paso $m$ después de convergencia se tiene que la muestra para $\\boldsymbol{\\theta}$ es $\\boldsymbol{\\theta}^{(m)}$. Entonces un valor predictivo puede ser obtenido a partir de la densidad posterior\n",
    "predictiva\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "f(y|\\mathbf{\\theta}^{(m)})\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Dentro del algoritmo de muestreo se procede así: en el paso $m$\n",
    "\n",
    "1. Se obtiene la muestra $\\mathbf{\\theta}^{(m)} \\sim p(\\mathbf{\\theta}|\\mathbf{y})$ (sección *model* de Stan).\n",
    "2. Se obtiene el valor predicho $y_{n+1} \\sim f(y|\\mathbf{\\theta}^{(m)})$ (sección *generated quantities* de Stan).\n",
    "\n",
    "Observe que en el numeral 2, se está usando la función de verosimilitud (que es una densidad) con parámetro $\\mathbf{\\theta}^{(m)}$.\n",
    "\n",
    "Cada observación $y_i$ en el caso de los modelos de regresión  depende no solamente del parámetro $\\mathbf{\\theta}$, sino también de un vector de variables regresoras o predictoras $\\mathbf{x}_i$. En este caso tenemos que la posterior predictiva es dada por \n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p(y_{n+1}|\\mathbf{y},x_{n+1}) = \\int\n",
    "f(y_{n+1}|\\mathbf{\\theta},x_{n+1})p(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta},\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "[Fuente:AP](https://github.com/AprendizajeProfundo/Estadistica-Bayesiana/blob/master/cuadernos/Distribucion_posterior_predictiva.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3e69d-8f04-4dc4-9177-4ac61208fb5a",
   "metadata": {},
   "source": [
    "#### Media Posterior\n",
    "La media posterior se encuentra entre la media a priori y el EMV de $\\theta$ si:\n",
    "\n",
    "$$E(\\theta|y)=w E(\\theta)+(1-w)\\hat{\\theta}$$\n",
    "$$0<w<1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fee1e1-f6b0-4c28-a32b-4e91e698fb12",
   "metadata": {},
   "source": [
    "### Ejemplos de modelos de regresión Bayesiana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5aa7fc-0ce9-4f76-a855-5682ad5529fb",
   "metadata": {},
   "source": [
    "#### Ejemplo 1: Modelo de regresión lineal Gaussiano \n",
    "\n",
    "En este caso el modelo estadístico es definido por\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[y_i] &= \\mu_i =  \\alpha +  \\mathbf{x_i}^T\\mathbf{\\beta} \\\\\n",
    "\\ln \\sigma_i &= \\eta +\\mathbf{z_i}^T\\mathbf{\\nu} \\\\\n",
    "\\alpha &\\sim \\mathcal{N}(\\mu_{\\alpha},\\sigma_{\\alpha}^2)\\\\\n",
    "\\mathbf{\\beta} &\\sim \\mathcal{N}(\\mathbf{\\mu}_{\\beta},\\rho_{\\beta}^2\\mathbf{I})\\\\\n",
    "\\eta &\\sim \\mathcal{N}(\\mu_{\\eta},\\sigma_{\\eta}^2)\\\\\n",
    "\\mathbf{\\nu} &\\sim \\mathcal{N}(\\mathbf{\\mu}_{\\nu},\\rho_{\\nu}^2\\mathbf{I})\\\\\n",
    "y_i &\\sim \\mathcal{N}(\\mu_i,\\sigma_i^2), \\hspace{3mm} i =1,\\ldots,n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "El parámetro del problema es $\\mathbf{\\theta} = (\\alpha,\\eta, \\mathbf{\\beta}^T,\\mathbf{\\nu}^T)^T$. \n",
    "\n",
    "En el caso homocedástico, se tiene que $\\sigma_i^2 = \\sigma^2$.\n",
    "\n",
    "#### Ejemplo 2: Modelo regresión Binomial\n",
    "\n",
    "La función *inv_logit* es definida por $\\text{inv_logit}(x) = (1+ \\exp (-x))^{-1}$. Esta es la función de distribución acumulada logística.\n",
    "\n",
    "El modelo Binomial de regresión se define por\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[y_i] &= \\mu_i= N_i\\pi_i\\\\\n",
    "\\pi_i &=  \\text{inv_logit}(\\alpha +  \\mathbf{x_i}^T\\mathbf{\\beta}) \\\\\n",
    "\\alpha &\\sim \\mathcal{N}(\\mu_{\\alpha},\\sigma_{\\alpha}^2)\\\\\n",
    "\\mathbf{\\beta} &\\sim \\mathcal{N}(\\mathbf{\\mu}_{\\beta},\\rho_{\\beta}^2\\mathbf{I})\\\\\n",
    "y_i &\\sim \\text{Binomial}(N_i,\\pi_i), \\hspace{3mm} i =1,\\ldots,n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Los valores $N_i$ se asumen conocidos. En este caso se tiene que $Var[y_i] = N_i\\pi_i(1-\\pi)$, por lo que $\\sigma_i =\\sqrt{N_i\\pi_i(1-\\pi)} $. \n",
    "\n",
    "El parámetro del problema es $\\mathbf{\\theta} = (\\alpha, \\mathbf{\\beta}^T)^T$.\n",
    "\n",
    "#### Ejemplo 3: Modelo de regresión Poisson\n",
    "\n",
    "El modelo Poisson de regresión se define por\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[y_i] &= \\mu_i\\\\\n",
    "\\ln \\mu_i&=  \\alpha +  \\mathbf{x_i}^T\\mathbf{\\beta}\\\\\n",
    "\\alpha &\\sim \\mathcal{N}(\\mu_{\\alpha},\\sigma_{\\alpha}^2)\\\\\n",
    "\\mathbf{\\beta} &\\sim \\mathcal{N}(\\mathbf{\\mu}_{\\beta},\\rho_{\\beta}^2\\mathbf{I})\\\\\n",
    "y_i &\\sim \\text{Poisson}(\\mu_i), \\hspace{3mm} i =1,\\ldots,n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "El parámetro del problema es $\\mathbf{\\theta} = (\\alpha, \\mathbf{\\beta}^T)^T$. Se tiene que $\\sigma_i = \\sqrt{\\mu_i}$\n",
    "\n",
    "#### Ejemplo 4: Modelo de regresión Beta (Beta-logit)\n",
    "\n",
    "Asumiremos que las observaciones son $y_i \\sim \\text{Beta}(a_i,b_i)$. \n",
    "\n",
    "Como hemos hecho antes usaremos la reparametrización\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_i &= \\frac{a_i}{a_i+b_i}\\\\\n",
    "\\phi_i &= a_i+b_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Observe que \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_i &= \\mu_i \\phi_i\\\\\n",
    "b_i &= \\phi_i(1-\\mu_i)\\\\\n",
    "\\sigma_i &= \\sqrt{\\frac{a_ib_i}{(a_i+b_i +1)(a_i+b_i)^2} } = \\sqrt{\\frac{\\mu_i(1-\\mu_i)}{1+\\phi_i}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Con esta parametrización es posible ahora definir el modelo de regresión Beta como\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[y_i] &= \\mu_i\\\\\n",
    " \\mu_i&=  \\text{inv_logit}(\\alpha +  \\mathbf{x_i}^T\\mathbf{\\beta})\\\\\n",
    "\\ln \\phi_i & = \\eta +  \\mathbf{z_i}^T\\mathbf{\\nu}\\\\\n",
    "\\alpha &\\sim \\mathcal{N}(\\mu_{\\alpha},\\sigma_{\\alpha}^2)\\\\\n",
    "\\mathbf{\\beta} &\\sim \\mathcal{N}(\\mathbf{\\mu}_{\\beta},\\rho_{\\beta}^2\\mathbf{I})\\\\\n",
    "\\eta &\\sim \\mathcal{N}(\\mu_{\\eta},\\sigma_{\\eta}^2)\\\\\n",
    "\\mathbf{\\nu} &\\sim \\mathcal{N}(\\mathbf{\\mu}_{\\nu},\\rho_{\\nu}^2\\mathbf{I})\\\\\n",
    "y_i &\\sim \\text{Beta}(\\mu_i\\phi_i,(1-\\mu_i)\\phi_i), \\hspace{3mm} i =1,\\ldots,n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "El parámetro del problema es $\\mathbf{\\theta} = (\\alpha,\\eta, \\mathbf{\\beta}^T,\\mathbf{\\nu}^T)^T$. Se tiene además que $\\sigma_i = \\sqrt{\\frac{\\mu_i(1-\\mu_i)}{1+\\phi_i}}$\n",
    "\n",
    "[Fuente: AP](https://github.com/AprendizajeProfundo/Estadistica-Bayesiana/blob/master/cuadernos/Modelos_Bayesianos_Regresion.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f97e8-f8a4-4814-b441-e497b910bf2e",
   "metadata": {},
   "source": [
    "[R](https://www.rpubs.com/kevortiz10/regresion-bayesiana)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0364f-79e0-4c3c-bae6-cdf72edb7404",
   "metadata": {},
   "source": [
    "### Ejemplo de implementación en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc89ba4-ad92-4ef2-a3b2-1874f21a3787",
   "metadata": {},
   "source": [
    "[Python](https://scikit-learn.org/stable/modules/linear_model.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea22b4e-f9f8-46b9-b7a9-8b1c926e10b5",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322398d2",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "Respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d68a7c-0e62-4595-8baf-83479b0dda53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6406dec-4320-40dc-948f-c57618151b19",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
